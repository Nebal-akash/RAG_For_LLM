# ğŸ§  Retrieval-Augmented Generation (RAG) with LangChain, ChromaDB, and Grok API

## ğŸ“˜ Project Overview
This project demonstrates how to build a **Retrieval-Augmented Generation (RAG)** system using **LangChain**, **ChromaDB**, and **Grok API**.  
The goal is to improve LLM responses by grounding them in **document-based context** (e.g., PDFs or research material).

---

## âš™ï¸ Workflow Summary
1. **PDF Data Extraction**  
   - Load and split PDF content using `langchain_community.document_loaders`.
   - Clean and preprocess text for embedding.

2. **Vector Embedding & Storage**  
   - Convert text chunks into vector embeddings.
   - Store vectors in **ChromaDB** for efficient retrieval.

3. **User Query Handling**  
   - Convert user input into a vector using the same embedding model.
   - Perform similarity search in the vector database.

4. **Response Generation**  
   - Combine retrieved context with the user query.
   - Pass the prompt to **Grok API (LLM)** to generate a factually grounded response.

---

## ğŸ§© Tech Stack

| Component | Technology |
|------------|-------------|
| **Framework** | LangChain |
| **Vector Database** | ChromaDB |
| **LLM Provider** | Grok API |
| **Embedding Source** | LangChain Embeddings |
| **Data Source** | PDF Documents |

---

## ğŸ§  Architecture Overview

```text
PDF â†’ Text Splitter â†’ Embeddings â†’ ChromaDB
                                    â†‘
                                   Query â†’ Embedding â†’ Similarity Search â†’ Context â†’ Grok LLM
